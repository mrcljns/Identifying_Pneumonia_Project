{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying pneumonia in X-ray images\n",
    "Maciej Lorens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from joblib import dump, load\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torchvision import  models, transforms\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    log_loss, \n",
    "    accuracy_score,\n",
    "    average_precision_score,\n",
    "    balanced_accuracy_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Hyperparameter optimization\n",
    "import optuna as opt\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "from skimage.io import imread\n",
    "import cv2\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "sn.set_theme(style=\"darkgrid\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve based on the provided folder or the latest folder from lightning logs\n",
    "def plot_loss_curve(fpath=None):\n",
    "    if fpath == None:\n",
    "        latest_file = os.path.join(max([os.path.join(\"/home/maciej/Uni/Studia II stopie≈Ñ/II rok/Machine Learning 2/GRU-ML-presentation/lightning_logs\", f) \n",
    "                                        for f in os.listdir(\"lightning_logs\")], \n",
    "                                        key=os.path.getctime), \"metrics.csv\")\n",
    "    else:\n",
    "        latest_file = os.path.join(fpath, \"metrics.csv\")\n",
    "    loss_data = pd.read_csv(latest_file)\n",
    "    loss_data['val_loss'] = loss_data.val_loss.shift(1)\n",
    "    loss_data = loss_data[[\"epoch\", \"train_loss\", \"val_loss\"]].dropna()\n",
    "    loss_data['epoch'] = loss_data.epoch\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(loss_data['epoch'], loss_data['train_loss'], label=\"Training loss\")\n",
    "    plt.plot(loss_data['epoch'], loss_data['val_loss'], label = \"Validation loss\")\n",
    "    plt.title(\"Loss curve\", fontsize=18)\n",
    "    plt.xlabel(\"Epoch\", fontsize=16)\n",
    "    plt.ylabel(\"Loss metric\", fontsize=16)\n",
    "    plt.xticks(loss_data['epoch'])\n",
    "    plt.legend(loc=\"upper right\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the images of X-rays\n",
    "data_dir = Path(\"chest_xray\")\n",
    "\n",
    "# Path to the train directory\n",
    "train_dir = data_dir / \"train\"\n",
    "\n",
    "# Path to test directory\n",
    "test_dir = data_dir / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dataframe of image paths and labels\n",
    "def get_image_df(img_dir):\n",
    "    # Paths to normal and pneumonia xrays\n",
    "    normal_dir = img_dir / \"NORMAL\"\n",
    "    pneumonia_dir = img_dir / \"PNEUMONIA\"\n",
    "\n",
    "    # List of all the images\n",
    "    normal_cases = normal_dir.glob(\"*.jpeg\")\n",
    "    pneumonia_cases = pneumonia_dir.glob(\"*.jpeg\")\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for img in normal_cases:\n",
    "        data.append((img, 0))\n",
    "\n",
    "    for img in pneumonia_cases:\n",
    "        data.append((img, 1))\n",
    "\n",
    "    data = pd.DataFrame(data, columns=[\"Image_dir\", \"Label\"])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_image_df(train_dir)\n",
    "test_data = get_image_df(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the counts for each class\n",
    "cases_count = train_data['Label'].value_counts()\n",
    "\n",
    "# Plot the results \n",
    "plt.figure(figsize=(10, 8))\n",
    "sn.barplot(x=cases_count.index, y=cases_count.values)\n",
    "plt.title('Number of cases', fontsize=14)\n",
    "plt.xlabel('Case type', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(range(len(cases_count.index)), ['Normal', 'Pneumonia'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get few samples for both the classes\n",
    "pneumonia_samples = (train_data[train_data['Label']==1]['Image_dir'].iloc[:3]).tolist()\n",
    "normal_samples = (train_data[train_data['Label']==0]['Image_dir'].iloc[:3]).tolist()\n",
    "\n",
    "# Concat the data in a single list\n",
    "samples = pneumonia_samples + normal_samples\n",
    "\n",
    "# Plot the data \n",
    "f, ax = plt.subplots(2, 3, figsize=(20, 10))\n",
    "for i in range(6):\n",
    "    img = imread(samples[i])\n",
    "    ax[i//3, i%3].imshow(img, cmap='gray')\n",
    "    if i<3:\n",
    "        ax[i//3, i%3].set_title(\"Normal\")\n",
    "    else:\n",
    "        ax[i//3, i%3].set_title(\"Pneumonia\")\n",
    "    ax[i//3, i%3].axis('off')\n",
    "    ax[i//3, i%3].set_aspect('auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting train data into train and validation set\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.3, shuffle=True, stratify=train_data[\"Label\"], random_state=42)\n",
    "train_data, val_data = train_data.reset_index(drop=True), val_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Pneumonia cases in the trainining set: {train_data['Label'].value_counts()[1]}, Normal cases in the trainining set: {train_data['Label'].value_counts()[0]}\")\n",
    "print(f\"Pneumonia cases in the validation set: {val_data['Label'].value_counts()[1]}, Normal cases in the validation set: {val_data['Label'].value_counts()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom PyTorch dataset of images\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data, transform, device=device):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_dir = self.data.Image_dir[index]\n",
    "        image = Image.open(image_dir)\n",
    "        label = self.data.Label[index]\n",
    "        image = self.transform(image)\n",
    "        # If image is greyscale then dstack\n",
    "        if image.shape[0] == 1:\n",
    "            image = torch.cat((image, image, image), dim=0)\n",
    "\n",
    "        return (image, torch.tensor(label).to(torch.float32).to(self.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Lightning dataloader\n",
    "class PneumoniaDataLoader(pl.LightningDataModule):\n",
    "    def __init__(self, train_data, val_data, test_data, random_state, batch_size, num_workers, device):\n",
    "        super().__init__()\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "        self.random_sate = random_state\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage):\n",
    "        # Data transformations for training\n",
    "        train_transforms = transforms.Compose([\n",
    "            transforms.Resize((150,150)),\n",
    "            transforms.RandomRotation(30),\n",
    "            transforms.RandomHorizontalFlip(p=0.4),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        # Data transformations for validation and testing\n",
    "        test_transforms = transforms.Compose([\n",
    "            transforms.Resize((150,150)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        # Create three instances of the custom dataset class\n",
    "        self.image_train = ImageDataset(self.train_data, transform=train_transforms)\n",
    "        self.image_val = ImageDataset(self.val_data, transform=test_transforms)\n",
    "        self.image_test = ImageDataset(self.test_data, transform=test_transforms)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # Return the dataloader of train data\n",
    "        return DataLoader(self.image_train\n",
    "                          , shuffle=True\n",
    "                          , batch_size=self.batch_size\n",
    "                          , drop_last=False\n",
    "                          , num_workers=self.num_workers\n",
    "                          , persistent_workers=True\n",
    "                          )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # Return the dataloader of validation data\n",
    "        return DataLoader(self.image_val\n",
    "                          , shuffle=False\n",
    "                          , batch_size=self.batch_size\n",
    "                          , drop_last=False\n",
    "                          , num_workers=self.num_workers\n",
    "                          , persistent_workers=True\n",
    "                          )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        # Return the dataloader of test data\n",
    "        return DataLoader(self.image_test\n",
    "                          , shuffle=False\n",
    "                          , batch_size=self.batch_size\n",
    "                          , drop_last=False\n",
    "                          , num_workers=self.num_workers\n",
    "                          , persistent_workers=True\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark simple CNN model class\n",
    "class BenchmarkCNNModel(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(18496, 120) # 16 * 34 * 34\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.softmax = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(self.dropout(F.relu(self.conv2(x))))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "\n",
    "# Transfer learning model with VGG16 for the stacking ensemble\n",
    "class VGG16Model(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.vgg16 = models.vgg16_bn(weights=torchvision.models.VGG16_BN_Weights.DEFAULT)\n",
    "        num_features = self.vgg16.classifier[-1].in_features\n",
    "        features = list(self.vgg16.classifier.children())[:-1]\n",
    "        self.fn = nn.Linear(num_features, 1)\n",
    "        self.vgg16.classifier = nn.Sequential(*features)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.vgg16(x)\n",
    "        x = self.fn(self.dropout(x))\n",
    "\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "\n",
    "# Transfer learning model with DenseNet169 for the stacking ensemble\n",
    "class DenseNetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.densenet = models.densenet169(weights=torchvision.models.DenseNet169_Weights.DEFAULT)\n",
    "        self.densenet.classifier = nn.Sequential(nn.Identity()) # or nn.Dropout(p=0.2), nn.Linear(num_features, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.densenet(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "# Meta Model for combining outputs of two other models\n",
    "class MetaModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fn1 = nn.Linear(2, 4)\n",
    "        self.fn2 = nn.Linear(4, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fn1(x))\n",
    "        x = self.relu(self.fn2(x))\n",
    "\n",
    "\n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pneumonia classification model created with lightning\n",
    "class PneumoniaModel(pl.LightningModule):\n",
    "    def __init__(self, model, lr=0.001):\n",
    "        super().__init__()\n",
    "        # Model of choice\n",
    "        self.model = model\n",
    "        # Learning rate\n",
    "        self.lr = lr\n",
    "        # Metrics\n",
    "        self.bce = nn.BCELoss()\n",
    "        self.acc = torchmetrics.Accuracy(task=\"binary\")\n",
    "        self.auroc = torchmetrics.AUROC(task=\"binary\")\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self.model(X)\n",
    "        train_loss = self.bce(y_pred.squeeze(), y)\n",
    "        self.log(\"train_loss\", train_loss, logger=True, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return train_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self.model(X)\n",
    "        val_loss = self.bce(y_pred.squeeze(), y)\n",
    "        self.log(\"val_loss\", val_loss, logger=True, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self.model(X)\n",
    "        test_bce = self.bce(y_pred.squeeze(), y)\n",
    "        test_acc = self.acc(y_pred.squeeze(), y.to(torch.int64))\n",
    "        test_roc = self.auroc(y_pred.squeeze(), y.to(torch.int64))\n",
    "        self.log_dict({\"BCE\": test_bce, \"Accuracy\": test_acc, \"ROC\": test_roc}, logger=True, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        # self.log(\"test_loss\", test_loss, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True)\n",
    "    \n",
    "    def predict_step(self, batch, batch_id):\n",
    "        X, y = batch\n",
    "        y_pred = self.model(X)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataloader\n",
    "pneumonia_loader = PneumoniaDataLoader(train_data=train_data\n",
    "                                       , val_data=val_data\n",
    "                                       , test_data=test_data\n",
    "                                       , random_state=42\n",
    "                                       , batch_size=128\n",
    "                                       , num_workers=8\n",
    "                                       , device=device\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42, workers=True)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"optuna\")\n",
    "\n",
    "def objective(trial):\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "    dropout = trial.suggest_categorical(\"dropout\", [0.2, 0.4])\n",
    "    lr = trial.suggest_categorical(\"lr\", [1e-3, 1e-2])\n",
    "\n",
    "    benchmark_model = BenchmarkCNNModel(dropout=dropout).to(device)\n",
    "    model = PneumoniaModel(benchmark_model, lr=lr)\n",
    "\n",
    "    optuna_loader = PneumoniaDataLoader(train_data=train_data\n",
    "                                        , val_data=val_data\n",
    "                                        , test_data=test_data\n",
    "                                        , random_state=42\n",
    "                                        , batch_size=batch_size\n",
    "                                        , num_workers=0\n",
    "                                        , device=device\n",
    "                                        )\n",
    "    \n",
    "    optuna_trainer = pl.Trainer(max_epochs=10\n",
    "                                , enable_checkpointing=False\n",
    "                                , logger=True\n",
    "                                , log_every_n_steps=1\n",
    "                                , check_val_every_n_epoch=1\n",
    "                                # , limit_train_batches=0.2\n",
    "                                # , limit_val_batches=0.2\n",
    "                                , deterministic=True\n",
    "                                , default_root_dir=\"benchmark_optuna_logs/\"\n",
    "                                , callbacks=[PyTorchLightningPruningCallback(trial, monitor=\"val_loss\"),\n",
    "                                             EarlyStopping(monitor=\"val_loss\", mode=\"min\")])\n",
    "    \n",
    "    hyperparameters = dict(batch_size=batch_size, dropout=dropout, lr=lr)\n",
    "    optuna_trainer.logger.log_hyperparams(hyperparameters)\n",
    "    optuna_trainer.fit(model, datamodule=optuna_loader)\n",
    "\n",
    "    return optuna_trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "study = opt.create_study(direction=\"minimize\", study_name=\"benchmark_model\", pruner=opt.pruners.MedianPruner(n_warmup_steps=5), load_if_exists=True)\n",
    "\n",
    "study.optimize(objective, n_trials=12, n_jobs=1)\n",
    "\n",
    "print(\"Best hyperparameters: \", study.best_params)\n",
    "print(\"Score on validation: \", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42, workers=True)\n",
    "benchmark_model = PneumoniaModel(BenchmarkCNNModel(dropout=0.2).to(device), lr=0.01)\n",
    "benchmark_trainer = pl.Trainer(max_epochs=20\n",
    "                               , check_val_every_n_epoch=1\n",
    "                               , log_every_n_steps=1\n",
    "                               , deterministic=True\n",
    "                               , default_root_dir=\"benchmark_logs/\"\n",
    "                               , callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_trainer.fit(benchmark_model, datamodule=pneumonia_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_trainer.test(benchmark_model, dataloaders=pneumonia_loader.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from the benchmark model\n",
    "benchmark_preds = benchmark_trainer.predict(benchmark_model, dataloaders=pneumonia_loader.test_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42, workers=True)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"optuna\")\n",
    "\n",
    "def objective(trial):\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "    dropout = trial.suggest_categorical(\"dropout\", [0.2, 0.4])\n",
    "    lr = trial.suggest_categorical(\"lr\", [1e-3, 1e-2])\n",
    "\n",
    "    vgg16_model = VGG16Model(dropout=dropout).to(device)\n",
    "    vgg16_model.vgg16.requires_grad_(False)\n",
    "    model = PneumoniaModel(vgg16_model, lr=lr)\n",
    "\n",
    "    optuna_loader = PneumoniaDataLoader(train_data=train_data\n",
    "                                        , val_data=val_data\n",
    "                                        , test_data=test_data\n",
    "                                        , random_state=42\n",
    "                                        , batch_size=batch_size\n",
    "                                        , num_workers=0\n",
    "                                        , device=device\n",
    "                                        )\n",
    "    \n",
    "    optuna_trainer = pl.Trainer(max_epochs=10\n",
    "                                , enable_checkpointing=False\n",
    "                                , logger=True\n",
    "                                , log_every_n_steps=1\n",
    "                                , check_val_every_n_epoch=1\n",
    "                                # , limit_train_batches=0.2\n",
    "                                # , limit_val_batches=0.2\n",
    "                                , deterministic=True\n",
    "                                , default_root_dir=\"vgg16_optuna_logs/\"\n",
    "                                , callbacks=[PyTorchLightningPruningCallback(trial, monitor=\"val_loss\"),\n",
    "                                             EarlyStopping(monitor=\"val_loss\", mode=\"min\")])\n",
    "    \n",
    "    hyperparameters = dict(batch_size=batch_size, dropout=dropout, lr=lr)\n",
    "    optuna_trainer.logger.log_hyperparams(hyperparameters)\n",
    "    optuna_trainer.fit(model, datamodule=optuna_loader)\n",
    "\n",
    "    return optuna_trainer.callback_metrics[\"val_loss\"].item()\n",
    "\n",
    "study = opt.create_study(direction=\"minimize\", study_name=\"vgg16_model\", pruner=opt.pruners.MedianPruner(n_warmup_steps=5), load_if_exists=True)\n",
    "\n",
    "study.optimize(objective, n_trials=12, n_jobs=1)\n",
    "\n",
    "print(\"Best hyperparameters: \", study.best_params)\n",
    "print(\"Score on validation: \", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42, workers=True)\n",
    "vgg16_model = VGG16Model(dropout=0.2).to(device)\n",
    "vgg16_model.vgg16.requires_grad_(False)\n",
    "\n",
    "vgg16_model_lightning = PneumoniaModel(vgg16_model, lr=0.02)\n",
    "vgg16_trainer = pl.Trainer(max_epochs=2\n",
    "                           , check_val_every_n_epoch=1\n",
    "                           , log_every_n_steps=2\n",
    "                           , deterministic=True\n",
    "                           , default_root_dir=\"vgg16_logs/\"\n",
    "                           , callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_trainer.fit(vgg16_model_lightning, datamodule=pneumonia_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_trainer.test(vgg16_model_lightning, dataloaders=pneumonia_loader.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from VGG16\n",
    "benchmark_preds = benchmark_trainer.predict(benchmark_model, dataloaders=pneumonia_loader.test_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Densenet169 and SVM Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special dataloaders for Densenet169 with SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.RandomHorizontalFlip(p=0.4),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create three instances of the custom dataset class\n",
    "image_train = ImageDataset(train_data, transform=train_transforms)\n",
    "image_val = ImageDataset(val_data, transform=train_transforms)\n",
    "image_test = ImageDataset(test_data, transform=test_transforms)\n",
    "\n",
    "train_loader = DataLoader(image_train\n",
    "                          , shuffle=False \n",
    "                          # Batch size doesn't matter here, since we're not training the CNN\n",
    "                          , batch_size=128\n",
    "                          , drop_last=False\n",
    "                          , num_workers=8\n",
    "                          )\n",
    "\n",
    "val_loader = DataLoader(image_val\n",
    "                        , shuffle=False\n",
    "                        # Batch size doesn't matter here, since we're not training the CNN\n",
    "                        , batch_size=128\n",
    "                        , drop_last=False\n",
    "                        , num_workers=8\n",
    "                        )\n",
    "\n",
    "test_loader = DataLoader(image_test\n",
    "                         , shuffle=False\n",
    "                         # Batch size doesn't matter here, since we're not training the CNN\n",
    "                         , batch_size=128\n",
    "                         , drop_last=False\n",
    "                         , num_workers=8\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize DenseNet169 with pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42, workers=True)\n",
    "densenet_model = DenseNetModel().to(device)\n",
    "densenet_model.densenet.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_densenet_predictions(dataloader, model, device):\n",
    "    model.eval()\n",
    "    cnn_preds_all = []\n",
    "\n",
    "    for X, _ in tqdm(dataloader, total=len(dataloader)):\n",
    "        cnn_preds = model(X)\n",
    "        cnn_preds_all += [cnn_preds.detach().cpu().numpy()]\n",
    "    \n",
    "    cnn_preds_all = np.concatenate(cnn_preds_all, axis=0)\n",
    "    \n",
    "    return cnn_preds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = get_densenet_predictions(train_loader, densenet_model, device)\n",
    "val_features = get_densenet_predictions(val_loader, densenet_model, device)\n",
    "test_features = get_densenet_predictions(test_loader, densenet_model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning LinearSVC hyperparameters with Optuna\n",
    "def objective(trial):\n",
    "    \n",
    "    params = dict(\n",
    "        C = trial.suggest_float(\"C\", 1e-2, 1e-1),\n",
    "        kernel = trial.suggest_categorical(\"kernel\", [\"linear\", \"rbf\"]),\n",
    "        class_weight = trial.suggest_categorical(\"class_weight\", [\"balanced\", None])\n",
    "    )\n",
    "\n",
    "    svc_model = SVC(**params, probability=True, random_state=42)\n",
    "\n",
    "    svc_pipe = Pipeline([(\"scaler\", StandardScaler())\n",
    "                     , (\"model\", svc_model)])\n",
    "\n",
    "    svc_pipe.fit(train_features, np.array(train_data[\"Label\"]))\n",
    "\n",
    "    score = log_loss(np.array(val_data[\"Label\"]), svc_pipe.predict(val_features))\n",
    "\n",
    "    return score\n",
    "\n",
    "study = opt.create_study(direction=\"minimize\", study_name=\"svc\", pruner=opt.pruners.HyperbandPruner())\n",
    "\n",
    "study.optimize(objective, n_trials=100, n_jobs=-1)\n",
    "\n",
    "print(\"Best hyperparameters: \", study.best_params)\n",
    "print(\"Score on validation: \", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = SVC(**study.best_params, probability=True, random_state=42)\n",
    "\n",
    "svc_pipe = Pipeline([(\"scaler\", StandardScaler())\n",
    "                    , (\"model\", svc_model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_pipe.fit(train_features, np.array(train_data[\"Label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_train_preds = svc_pipe.predict(train_features)\n",
    "svc_val_preds = svc_pipe.predict(val_features)\n",
    "svc_test_preds = svc_pipe.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training set ROC-AUC score: {roc_auc_score(svc_train_preds, np.array(train_data['Label']))}\")\n",
    "print(f\"Training set balanced accuracy: {balanced_accuracy_score(svc_train_preds, np.array(train_data['Label']))}\")\n",
    "print(f\"Training set f1-score: {f1_score(svc_train_preds, np.array(train_data['Label']))}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Validation set ROC-AUC score: {roc_auc_score(svc_val_preds, np.array(val_data['Label']))}\")\n",
    "print(f\"Validation set balanced accuracy: {balanced_accuracy_score(svc_val_preds, np.array(val_data['Label']))}\")\n",
    "print(f\"Validation set f1-score: {f1_score(svc_val_preds, np.array(val_data['Label']))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix of SVC predictions on the validation set\n",
    "conf_matrix = confusion_matrix(np.array(val_data['Label']), svc_val_preds, labels=[0, 1])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[\"Normal\", \"Pneumonia\"])\n",
    "disp.plot()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model pipeline\n",
    "dump(svc_pipe, 'svc_pipeline.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking of the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best benchmark CNN model\n",
    "best_benchmark = PneumoniaModel.load_from_checkpoint(checkpoint_path=\"lightning_logs/version_10/checkpoints/epoch=1-step=58.ckpt\", model=BenchmarkCNNModel(dropout=0.2), lr=0.02)\n",
    "\n",
    "# Load best vgg16 model\n",
    "best_vgg16 = PneumoniaModel.load_from_checkpoint(checkpoint_path=\"lightning_logs/version_10/checkpoints/epoch=1-step=58.ckpt\", model=VGG16Model(dropout=0.2), lr=0.02)\n",
    "\n",
    "# Load SVM pipeline\n",
    "svc_pipe = load(\"svc_pipeline.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the input data for the meta-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions of Densenet+SVM and VGG16 as input for the meta-learner\n",
    "def get_meta_data(image_df):\n",
    "    image_transforms = transforms.Compose([\n",
    "        transforms.Resize((150,150)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    meta_loader = DataLoader(ImageDataset(image_df, transform=image_transforms)\n",
    "                             , shuffle=False\n",
    "                             # Batch size doesn't matter here, since we're not training the CNN\n",
    "                             , batch_size=32\n",
    "                             , drop_last=False\n",
    "                             , num_workers=8\n",
    "                             )\n",
    "    \n",
    "    vgg16_preds = vgg16_trainer.predict(best_vgg16, dataloaders=meta_loader)\n",
    "    vgg16_preds = np.hstack([pred.squeeze(1).detach().cpu().numpy() for pred in vgg16_preds])\n",
    "    \n",
    "    svm_features = get_densenet_predictions(meta_loader, densenet_model, device)\n",
    "\n",
    "    svm_preds = svc_pipe.predict_proba(svm_features)[:, 1]\n",
    "\n",
    "    return np.concatenate((vgg16_preds.reshape(-1, 1), svm_preds.reshape(-1, 1)), axis=1), np.array(image_df[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_meta_data(train_data)\n",
    "X_val, y_val = get_meta_data(val_data)\n",
    "X_test, y_test = get_meta_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.from_numpy(X_train).unsqueeze(1).to(torch.float32).to(device), torch.from_numpy(y_train).to(torch.float32).to(device))\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=128)\n",
    "\n",
    "val_dataset = TensorDataset(torch.from_numpy(X_val).unsqueeze(1).to(torch.float32).to(device), torch.from_numpy(y_val).to(torch.float32).to(device))\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=128)\n",
    "\n",
    "test_dataset = TensorDataset(torch.from_numpy(X_test).unsqueeze(1).to(torch.float32).to(device), torch.from_numpy(y_test).to(torch.float32).to(device))\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the meta-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42, workers=True)\n",
    "meta_model = MetaModel().to(device)\n",
    "\n",
    "meta_model_lightning = PneumoniaModel(meta_model, lr=0.05)\n",
    "meta_trainer = pl.Trainer(max_epochs=20, check_val_every_n_epoch=1, log_every_n_steps=2, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_trainer.fit(meta_model_lightning, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_trainer.test(meta_model_lightning, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_preds = vgg16_trainer.predict(best_vgg16, dataloaders=pneumonia_loader.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_preds = np.hstack([pred.squeeze(1).detach().cpu().numpy() for pred in vgg16_preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_preds = meta_trainer.predict(meta_model_lightning, dataloaders=test_loader)\n",
    "meta_preds = np.hstack([pred.squeeze(1, 2).detach().cpu().numpy() for pred in meta_preds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC CURVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve\n",
    "fpr_vgg16, tpr_vgg16, _ = roc_curve(y_test, vgg16_preds)\n",
    "fpr_svc, tpr_svc, _ = roc_curve(y_test, svc_test_preds)\n",
    "fpr_meta, tpr_meta, thresholds = roc_curve(y_test, meta_preds)\n",
    "\n",
    "# Calculate ROC-AUC score\n",
    "roc_vgg16 = roc_auc_score(y_test, vgg16_preds)\n",
    "roc_svc = roc_auc_score(y_test, svc_test_preds)\n",
    "roc_meta = roc_auc_score(y_test, meta_preds)\n",
    "\n",
    "# Visualize ROC curve\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(fpr_vgg16, tpr_vgg16, color='red', lw=2, label=f'VGG16 ROC (AUC = {roc_vgg16:.2f})')\n",
    "plt.plot(fpr_svc, tpr_svc, color='orange', lw=2, label=f'SVM ROC (AUC = {roc_svc:.2f})')\n",
    "plt.plot(fpr_meta, tpr_meta, color='blue', lw=2, label=f'Stacking ROC (AUC = {roc_meta:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - ANN, TCN, LSTM')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
